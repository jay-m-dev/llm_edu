#!/usr/bin/env python3
import json
import subprocess
import sys

BODIES = {
    "Define core game fantasy": """### Goal\nDefine a concise player fantasy that frames the experience and anchors the game’s tone and mechanics.\n\n### Subtasks\n- Write a 1–2 sentence fantasy statement that fits on a single UI panel.\n- List 3 core player verbs (e.g., tune, test, inspect) as a short bullet list.\n- Map each verb to a concrete simulation mechanic in a short table.\n- Add a 1–2 sentence tone statement (“playful on the surface, serious underneath”).\n\n### Acceptance Criteria\n- A single fantasy statement exists and is under 40 words.\n- Exactly 3 player verbs are listed and mapped to mechanics.\n- The tone statement is present and uses non-punitive language.\n- No references to real ML training or backend systems appear.\n""",
    "Define MVP core gameplay loop": """### Goal\nDescribe the repeatable loop the player performs so gameplay is clear and teachable.\n\n### Subtasks\n- Write a 4–6 step loop description using numbered steps.\n- Specify player inputs and simulation outputs for each step.\n- Add a short “failure happens here” note for one step.\n- Provide a 1-sentence summary of the loop for onboarding.\n\n### Acceptance Criteria\n- Loop is 4–6 steps and readable without extra context.\n- Each step lists at least one input and one output.\n- Failure is explicitly mentioned in the loop.\n- Summary sentence is under 25 words.\n""",
    "Define success and failure philosophy": """### Goal\nSet clear, non-punitive success and failure framing to guide UX and scenario design.\n\n### Subtasks\n- Write a 2–3 sentence philosophy statement covering success and failure.\n- Define 3 named failure types with a 1-line learning takeaway each.\n- Draft 2 example failure messages in the intended tone.\n- Add 2 example success messages in the same tone.\n\n### Acceptance Criteria\n- Philosophy statement exists and is under 80 words.\n- Three failure types are listed with learning takeaways.\n- Two failure and two success messages are present.\n- Messaging avoids blame and emphasizes experimentation.\n""",
    "Define MVP scope boundaries": """### Goal\nMake a clear, written boundary for what the MVP includes and excludes.\n\n### Subtasks\n- Create an “In Scope” list with 6–10 items.\n- Create an “Out of Scope” list with 6–10 items.\n- Add a one-line rationale for each out-of-scope item.\n- Place the lists in a single markdown doc.\n\n### Acceptance Criteria\n- Both lists exist and are non-overlapping.\n- Out-of-scope list includes real ML training and multiplayer.\n- Each out-of-scope item has a rationale line.\n- Document is 1 page or less.\n""",
    "Implement simplified tokenization system": """### Goal\nProvide deterministic tokenization so players can inspect how text becomes tokens.\n\n### Subtasks\n- Implement a tokenizer that splits into word and punctuation tokens.\n- Add unit tests covering whitespace, punctuation, and empty input.\n- Expose tokens in simulation state for UI rendering.\n- Create a minimal UI view that lists tokens in order.\n\n### Acceptance Criteria\n- Same input always yields the same token sequence.\n- Tests cover at least 5 tokenization cases.\n- Tokens are available in simulation state.\n- UI shows tokens in order with indexes.\n""",
    "Implement pseudo-embedding representation": """### Goal\nCreate a simple, visual-friendly representation of token “meaning.”\n\n### Subtasks\n- Define a deterministic mapping from token text to 3–5 numeric features.\n- Store the feature vector for each token in simulation state.\n- Add a small UI element that visualizes the features (bars or chips).\n- Add tests for stability of the mapping.\n\n### Acceptance Criteria\n- Each token has a stable feature vector across runs.\n- Feature vectors are stored and retrievable per token.\n- UI displays a visual representation for selected tokens.\n- Tests confirm deterministic mapping.\n""",
    "Implement attention approximation model": """### Goal\nSimulate attention weights with a simple heuristic for educational visualization.\n\n### Subtasks\n- Define a heuristic (e.g., recency + keyword match) in code.\n- Compute attention weights per generation step.\n- Normalize weights for visualization.\n- Expose weights in simulation state for UI use.\n\n### Acceptance Criteria\n- Weights are produced for every generated token.\n- Weights are deterministic for identical inputs and seed.\n- Weights sum to 1.0 (or an explicitly defined scale).\n- Data is available for the attention visualization.\n""",
    "Implement multi-layer transformation pipeline": """### Goal\nModel a simple multi-stage transformation to mimic depth without real ML math.\n\n### Subtasks\n- Implement 3 named transformation stages as pure functions.\n- Run stages in order to produce output tokens.\n- Capture intermediate outputs per stage in state.\n- Add a UI toggle to show each stage output.\n\n### Acceptance Criteria\n- Output changes when any single stage is bypassed.\n- Intermediate outputs are stored per stage.\n- UI can switch between stage outputs.\n- Pipeline is deterministic for same inputs.\n""",
    "Implement context window mechanics": """### Goal\nSimulate a limited context window and show what gets forgotten.\n\n### Subtasks\n- Implement a max context size setting in the simulator.\n- Truncate oldest tokens when the limit is exceeded.\n- Track dropped tokens in state.\n- Add a UI view showing active vs dropped tokens.\n\n### Acceptance Criteria\n- Tokens beyond the limit are removed in order.\n- Dropped tokens are recorded with their original positions.\n- UI clearly distinguishes active vs dropped tokens.\n- Behavior is deterministic with the same input.\n""",
    "Implement sampling logic": """### Goal\nSimulate sampling behavior with temperature and randomness controls.\n\n### Subtasks\n- Implement temperature and randomness parameters in the generator.\n- Use a seeded RNG for deterministic sampling.\n- Connect sampling to the probability distribution data.\n- Provide UI inputs for temperature and randomness.\n\n### Acceptance Criteria\n- Same seed and settings yield identical output.\n- Changing temperature changes token variability.\n- Sampling chooses tokens consistent with the distribution.\n- UI controls update the simulation parameters.\n""",
    "Implement deterministic replay support": """### Goal\nAllow any run to be replayed exactly to reinforce learning.\n\n### Subtasks\n- Record seed, inputs, parameters, and step data for a run.\n- Persist the recording locally (e.g., localStorage).\n- Implement a replay mode that replays recorded steps.\n- Add a UI button to replay the last run.\n\n### Acceptance Criteria\n- Replay produces identical output and visuals.\n- Recordings can be saved and loaded locally.\n- Replay does not re-randomize sampling.\n- UI clearly indicates when replay mode is active.\n""",
    "Implement token-by-token generation": """### Goal\nGenerate output incrementally so players can see the model “think.”\n\n### Subtasks\n- Implement a stepwise generation loop.\n- Emit a token event after each step.\n- Add Play/Pause/Step controls in the UI.\n- Show current step number and current token.\n\n### Acceptance Criteria\n- Tokens appear one at a time in the UI.\n- Pause and Step controls work without losing state.\n- Current step and token are visible during generation.\n- No full-output burst occurs without stepping.\n""",
    "Implement per-token probability tracking": """### Goal\nTrack and expose per-token probabilities for visualization and learning.\n\n### Subtasks\n- Create a mock probability distribution for each generation step.\n- Store the distribution with the generated token.\n- Expose distributions in simulation state.\n- Add tests to ensure distributions are valid.\n\n### Acceptance Criteria\n- Each generated token has an associated distribution.\n- Distributions are normalized or explicitly scaled.\n- State exposes distributions for UI use.\n- Tests verify determinism and validity.\n""",
    "Implement attention weight tracking": """### Goal\nStore attention weights per step so the UI can visualize focus.\n\n### Subtasks\n- Capture attention weights for each generated token.\n- Store weights indexed by source token ID.\n- Include attention data in replay recordings.\n- Add a small debug view listing weights for the current step.\n\n### Acceptance Criteria\n- Attention weights exist for every generated token.\n- Weights are linked to source tokens by ID.\n- Replay uses the recorded attention data.\n- Debug view shows weights for the current step.\n""",
    "Implement generation replay mode": """### Goal\nProvide a player-facing replay mode to review a completed run step-by-step.\n\n### Subtasks\n- Add replay controls (Play/Pause/Step/Speed).\n- Sync replay position with token output and visualizations.\n- Allow selection of the last saved run.\n- Add a replay banner or status indicator.\n\n### Acceptance Criteria\n- Replay controls affect the replay timeline.\n- Tokens and visualizations stay in sync during replay.\n- Player can exit replay without losing the run.\n- Replay status is clearly visible.\n""",
    "Implement objective system": """### Goal\nProvide a simple objective framework to give scenarios clear goals.\n\n### Subtasks\n- Define an objective schema (id, description, evaluate).\n- Implement evaluation at end of a run.\n- Add at least 3 objectives tied to LLM concepts.\n- Display objective status in the UI.\n\n### Acceptance Criteria\n- Objectives are evaluated deterministically.\n- UI shows pass/fail for each objective.\n- At least 3 objectives are available.\n- Objectives require no external services.\n""",
    "Implement failure conditions and diagnostics": """### Goal\nDetect common failure modes and explain them clearly to the player.\n\n### Subtasks\n- Define 3–5 failure conditions in code.\n- Implement detection logic per condition.\n- Create a diagnostics panel with cause and hint.\n- Link each failure to a short learning takeaway.\n\n### Acceptance Criteria\n- Failures trigger reliably and deterministically.\n- Diagnostics show cause and next action.\n- Tone is non-punitive and instructional.\n- Diagnostics appear in replay mode.\n""",
    "Implement scoring dimensions": """### Goal\nProvide multi-axis scoring to help players compare runs.\n\n### Subtasks\n- Define 3–4 scoring dimensions with formulas.\n- Implement scoring at run completion.\n- Show a score breakdown panel.\n- Store scores with run records.\n\n### Acceptance Criteria\n- Scores are deterministic for the same run.\n- Each dimension has a visible value.\n- Score panel appears after a run completes.\n- Scores are stored for replay and comparison.\n""",
    "Implement unlockable parameters": """### Goal\nGate advanced controls to create progression without adding new systems.\n\n### Subtasks\n- Define unlock rules based on objective completion.\n- Mark advanced parameters as locked by default.\n- Persist unlock state locally.\n- Add a small unlock notification.\n\n### Acceptance Criteria\n- Locked parameters are visibly disabled.\n- Completing an objective unlocks the intended parameter.\n- Unlocks persist after refresh.\n- Notification appears once per unlock.\n""",
    "Implement sandbox mode": """### Goal\nAllow free experimentation with all controls and no objectives.\n\n### Subtasks\n- Add a sandbox toggle in the UI.\n- Disable objectives and scoring when sandbox is on.\n- Unlock all parameters in sandbox.\n- Display a sandbox banner.\n\n### Acceptance Criteria\n- Sandbox toggle switches modes immediately.\n- Objectives and scoring are hidden or inactive.\n- All controls are enabled in sandbox.\n- Sandbox status is clearly visible.\n""",
    "Implement token probability visualization": """### Goal\nShow probability distributions so players can see why a token was chosen.\n\n### Subtasks\n- Create a compact probability list or bar chart component.\n- Render the top N probabilities for the current step.\n- Highlight the chosen token.\n- Update the view on every token step.\n\n### Acceptance Criteria\n- Probabilities are visible and labeled.\n- Chosen token is clearly highlighted.\n- View updates every generation step.\n- Component works on mobile widths.\n""",
    "Implement attention heatmap visualization": """### Goal\nVisualize attention weights as a heatmap to show focus over input tokens.\n\n### Subtasks\n- Build a heatmap grid component.\n- Map weights to color intensity.\n- Add hover/focus to show exact values.\n- Sync the heatmap to the current generation step.\n\n### Acceptance Criteria\n- Heatmap renders for the current step.\n- Color intensity reflects weight values.\n- Hover/focus shows numeric weights.\n- Heatmap updates when the step changes.\n""",
    "Implement context window visualization": """### Goal\nMake the active context window visible so players can see forgetting in action.\n\n### Subtasks\n- Render tokens in a horizontal or vertical list.\n- Mark the active window bounds visually.\n- Gray out or strike dropped tokens.\n- Update the view when truncation occurs.\n\n### Acceptance Criteria\n- Active window bounds are visible at all times.\n- Dropped tokens are clearly distinguished.\n- View updates on every truncation event.\n- Layout remains readable on mobile.\n""",
    "Implement hallucination indicators": """### Goal\nFlag likely hallucinations to teach the tradeoff between creativity and accuracy.\n\n### Subtasks\n- Define a deterministic hallucination heuristic.\n- Mark tokens or spans that trigger the heuristic.\n- Add a tooltip explaining the likely cause.\n- Add a toggle to show/hide indicators.\n\n### Acceptance Criteria\n- Indicators appear only when heuristic triggers.\n- Tooltip explains the reason in 1–2 sentences.\n- Toggle hides and shows indicators instantly.\n- Indicators are preserved in replay.\n""",
    "Implement live parameter sliders": """### Goal\nProvide live controls for key parameters to enable experimentation.\n\n### Subtasks\n- Add sliders for temperature, randomness, and context size.\n- Show numeric values next to each slider.\n- Apply changes to subsequent generation steps.\n- Persist last-used values locally.\n\n### Acceptance Criteria\n- Slider changes affect output without reload.\n- Values are clamped to safe ranges.\n- Numeric readouts match slider positions.\n- Values persist across refresh.\n""",
    "Define playful visual language": """### Goal\nEstablish a consistent visual and writing style that feels playful but credible.\n\n### Subtasks\n- Choose 1–2 fonts and a primary color palette.\n- Define UI tone words and example phrases.\n- Create a small style guide markdown file.\n- Include examples for buttons, panels, and warnings.\n\n### Acceptance Criteria\n- Style guide exists in the repo.\n- Fonts and colors are explicitly listed.\n- Tone examples cover success and failure states.\n- Guidance is concise and usable by developers.\n""",
    "Implement explanation panel": """### Goal\nProvide optional, concise explanations of mechanics without blocking play.\n\n### Subtasks\n- Build a collapsible panel component.\n- Write 5–8 short explanations for key mechanics.\n- Link panel content to the selected UI element.\n- Add a keyboard shortcut to toggle the panel.\n\n### Acceptance Criteria\n- Panel opens and closes reliably.\n- Explanations are visible and under 3 sentences each.\n- Content updates when the focus changes.\n- Panel does not cover primary controls.\n""",
    "Implement tooltips and hover hints": """### Goal\nAdd lightweight hints so players can learn without leaving the UI.\n\n### Subtasks\n- Build a reusable tooltip component.\n- Add tooltips to at least 10 labeled controls.\n- Ensure tooltips appear on hover and keyboard focus.\n- Keep tooltip copy to one sentence each.\n\n### Acceptance Criteria\n- Tooltips appear on hover and focus.\n- At least 10 UI elements have tooltips.\n- Tooltip text is concise and educational.\n- Tooltips do not obstruct critical controls.\n""",
    "Implement failure feedback UI": """### Goal\nDisplay clear, non-punitive feedback when a failure occurs.\n\n### Subtasks\n- Create a failure banner or panel component.\n- Display failure name, cause, and a next-step hint.\n- Add a “Try Again” button that resets the run.\n- Style the UI to feel encouraging.\n\n### Acceptance Criteria\n- Failure UI appears immediately on failure.\n- Cause and hint are visible without scrolling.\n- Try Again resets the run state.\n- Copy avoids blame and uses supportive tone.\n""",
    "Implement save and load system": """### Goal\nAllow players to save and restore runs and settings locally.\n\n### Subtasks\n- Define a JSON save schema for runs and settings.\n- Implement save/load to localStorage.\n- Build a save/load UI with list and load buttons.\n- Handle corrupted or missing saves gracefully.\n\n### Acceptance Criteria\n- Saves can be created and loaded.\n- Loaded state matches the original run.\n- Corrupted saves show an error message.\n- No backend services are used.\n""",
    "Implement scenario state persistence": """### Goal\nPersist scenario progress so players can resume where they left off.\n\n### Subtasks\n- Store scenario progress in localStorage.\n- Restore scenario state on app load.\n- Add a “Reset Scenario” button.\n- Ensure scenario persistence works with save/load.\n\n### Acceptance Criteria\n- Scenario progress resumes after refresh.\n- Reset clears scenario progress immediately.\n- Persistence works without affecting other scenarios.\n- No server calls are made.\n""",
    "Implement parameter presets": """### Goal\nLet players save and apply sets of parameters quickly.\n\n### Subtasks\n- Define a preset data structure.\n- Build UI to save the current parameters as a preset.\n- Add 3 default presets.\n- Allow applying and deleting presets.\n\n### Acceptance Criteria\n- Presets apply all parameters at once.\n- Default presets are available on first run.\n- Presets can be deleted.\n- Presets persist across refresh.\n""",
    "Create intro scenario": """### Goal\nCreate a short onboarding scenario that teaches the core loop.\n\n### Subtasks\n- Define scenario setup (prompt, parameters, objectives).\n- Implement the scenario as a selectable entry.\n- Add one objective and one likely failure.\n- Write a 2–3 sentence intro text.\n\n### Acceptance Criteria\n- Scenario is playable from the scenario list.\n- Objective is visible and trackable.\n- Failure can occur and is explained.\n- Intro text is under 60 words.\n""",
    "Create context window failure scenario": """### Goal\nTeach how context limits cause forgetting and failure.\n\n### Subtasks\n- Create a scenario with a prompt that exceeds the context window.\n- Set a small context size in the scenario config.\n- Add an objective that fails due to truncation.\n- Provide a hint that mentions context size.\n\n### Acceptance Criteria\n- Failure occurs because earlier tokens are dropped.\n- Diagnostics point to context truncation.\n- Player can succeed by increasing context or shortening input.\n- Scenario runs deterministically with a fixed seed.\n""",
    "Create hallucination tradeoff scenario": """### Goal\nShow how higher randomness can increase hallucinations and reduce accuracy.\n\n### Subtasks\n- Define a scenario with an accuracy-focused objective.\n- Set high randomness/temperature in the default parameters.\n- Add a hint encouraging parameter adjustment.\n- Enable hallucination indicators for the scenario.\n\n### Acceptance Criteria\n- High randomness triggers hallucination indicators.\n- Reducing randomness improves objective success rate.\n- Objective and hints are visible in the UI.\n- Scenario is deterministic with a fixed seed.\n""",
    "Create prompt sensitivity scenario": """### Goal\nDemonstrate how small prompt changes lead to different outputs.\n\n### Subtasks\n- Create two near-identical prompts in the scenario.\n- Add a toggle to switch between them.\n- Define an objective that requires stabilizing output.\n- Add a short hint about prompt specificity.\n\n### Acceptance Criteria\n- Switching prompts changes output noticeably.\n- Objective is achievable with prompt adjustments.\n- Toggle is available in the scenario UI.\n- Scenario is deterministic with a fixed seed.\n""",
    "Define frontend architecture": """### Goal\nDocument a clear frontend structure so implementation is consistent and modular.\n\n### Subtasks\n- Choose the framework and state management approach in writing.\n- Define folder structure and core component boundaries.\n- Document data flow between UI and simulation engine.\n- Add the architecture document to the repo.\n\n### Acceptance Criteria\n- Architecture doc exists as markdown.\n- Framework and state approach are explicitly named.\n- Component boundaries and data flow are described.\n- No backend services are referenced.\n""",
    "Define simulation engine boundaries": """### Goal\nSpecify what the simulation engine owns versus the UI to avoid mixed concerns.\n\n### Subtasks\n- Write a small API surface for the engine (functions and data).\n- Define engine inputs and outputs in a table.\n- List responsibilities that explicitly belong to UI only.\n- Add the boundary doc to the repo.\n\n### Acceptance Criteria\n- Boundary document exists in markdown.\n- Engine API surface is listed and minimal.\n- UI-only responsibilities are explicit.\n- Determinism requirement is stated.\n""",
    "Implement deterministic simulation clock": """### Goal\nProvide a deterministic timing system for stepping the simulation.\n\n### Subtasks\n- Implement a tick-based clock service.\n- Drive generation steps off the clock ticks.\n- Add pause, resume, and step controls.\n- Add a speed control that scales tick rate deterministically.\n\n### Acceptance Criteria\n- One tick equals one simulation step.\n- Pause and step work reliably.\n- Speed control affects tick rate without changing outputs.\n- Same seed yields the same sequence of steps.\n""",
    "Implement performance constraints": """### Goal\nKeep the simulator responsive by enforcing safe limits.\n\n### Subtasks\n- Define limits for max tokens and max steps per run.\n- Enforce limits in the simulation loop.\n- Show a warning when limits are hit.\n- Allow sandbox mode to override limits.\n\n### Acceptance Criteria\n- Limits are enforced consistently.\n- Warning message appears when a limit is hit.\n- Sandbox mode can bypass limits.\n- App remains responsive under stress.\n""",
    "Implement onboarding flow": """### Goal\nGuide new players through the basics without long explanations.\n\n### Subtasks\n- Implement a 3–4 step onboarding modal sequence.\n- Provide skip and exit options.\n- Link onboarding completion to the intro scenario.\n- Store completion state locally.\n\n### Acceptance Criteria\n- Onboarding shows on first visit only.\n- Users can skip at any time.\n- Completion state persists across refresh.\n- Flow ends at the intro scenario.\n""",
    "Implement settings panel": """### Goal\nProvide a central place for basic app settings.\n\n### Subtasks\n- Build a settings panel UI.\n- Add toggles for hints, animation speed, and sound (even if sound is no-op).\n- Persist settings in localStorage.\n- Apply setting changes immediately.\n\n### Acceptance Criteria\n- Settings panel is accessible from the main UI.\n- Settings persist across refresh.\n- Changes apply without reload.\n- Default settings are sensible and documented in code.\n""",
    "Implement error handling": """### Goal\nPrevent crashes from breaking the experience and provide recovery paths.\n\n### Subtasks\n- Add a top-level error boundary.\n- Create a friendly error screen with a reset button.\n- Log errors to the console with context.\n- Handle simulation errors with a non-blocking banner.\n\n### Acceptance Criteria\n- UI does not white-screen on handled errors.\n- Error screen offers a reset action.\n- Errors are logged with a short message.\n- Simulation errors show a banner without stopping the app.\n""",
    "Add MVP instrumentation": """### Goal\nAdd lightweight local instrumentation for debugging and iteration.\n\n### Subtasks\n- Define 6–10 local event types.\n- Implement a simple event logger.\n- Add a setting to enable/disable logging.\n- Provide a local log viewer panel.\n\n### Acceptance Criteria\n- Events are recorded with timestamps.\n- Logging can be toggled off.\n- Logs are viewable in the UI.\n- No network calls are made.\n""",
    "Prepare MVP build configuration": """### Goal\nEnsure the project builds reliably for deployment.\n\n### Subtasks\n- Configure production build settings.\n- Add a build script to package.json.\n- Document build and preview steps in README.\n- Verify build output runs locally.\n\n### Acceptance Criteria\n- `npm run build` completes successfully.\n- README includes build and preview commands.\n- Build output runs without server dependencies.\n- Build is deterministic across runs.\n""",
    "Define explicit non-goals": """### Goal\nMake the project’s exclusions explicit to prevent scope creep.\n\n### Subtasks\n- Write a “Non-Goals” section in a markdown doc.\n- List at least 8 exclusions.\n- Add a one-line rationale for each exclusion.\n- Link the section from the README.\n\n### Acceptance Criteria\n- Non-Goals section exists and is easy to find.\n- List includes training, inference, multiplayer, accounts, and backend services.\n- Each non-goal has a rationale line.\n- README links to the Non-Goals section.\n""",
}


def main():
    cmd = ["gh", "issue", "list", "--limit", "200", "--json", "number,title"]
    result = subprocess.run(cmd, check=False, capture_output=True, text=True)
    if result.returncode != 0:
        print(result.stderr.strip(), file=sys.stderr)
        return result.returncode

    issues = {item["title"]: item["number"] for item in json.loads(result.stdout)}
    missing = [title for title in BODIES if title not in issues]
    if missing:
        print("Missing issues for titles:", ", ".join(missing), file=sys.stderr)
        return 1

    for title, body in BODIES.items():
        number = issues[title]
        edit_cmd = ["gh", "issue", "edit", str(number), "--body", body]
        edit_result = subprocess.run(edit_cmd, check=False, capture_output=True, text=True)
        if edit_result.returncode != 0:
            print(edit_result.stderr.strip(), file=sys.stderr)
            return edit_result.returncode

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
