#!/usr/bin/env python3
import json
import subprocess
import sys

BREAKDOWNS = {
    "Define core game fantasy": """### Subtasks\n- Draft a 1-2 paragraph fantasy statement that frames the player as an LLM architect.\n- Identify core player verbs and tone words (playful surface, serious systems).\n- Map 3-5 game mechanics to real LLM concepts.\n- Review against MVP constraints and remove out-of-scope elements.\n\n### Acceptance Criteria\n- A concise fantasy statement exists and fits on one screen.\n- Player role and core verbs are explicit.\n- Each mechanic maps to a concrete LLM concept.\n- No implied features beyond MVP scope.\n""",
    "Define MVP core gameplay loop": """### Subtasks\n- Outline the loop steps (experiment -> run -> observe -> adjust).\n- Define player inputs and sim outputs at each step.\n- Identify where failure happens and how feedback is delivered.\n- Write a short loop description for in-game use.\n\n### Acceptance Criteria\n- Loop is 4-6 steps and readable in under 10 seconds.\n- Failure and feedback are explicit parts of the loop.\n- Steps map to current MVP mechanics only.\n- Deterministic simulation assumption is stated.\n""",
    "Define success and failure philosophy": """### Subtasks\n- Define what success means in scenarios (learning outcomes, not perfect output).\n- List 3-5 failure types and what they teach.\n- Draft tone guidelines for feedback copy.\n- Provide 2 example failure messages.\n\n### Acceptance Criteria\n- Success/failure definitions are concise and actionable.\n- At least 3 failure types are described with learning value.\n- Feedback tone is non-punitive and encouraging.\n- Example messages align with the tone guidelines.\n""",
    "Define MVP scope boundaries": """### Subtasks\n- Create a clear in-scope list of MVP features.\n- Create a clear out-of-scope list.\n- Add a short rationale for each out-of-scope item.\n- Cross-check against project constraints and adjust.\n\n### Acceptance Criteria\n- In-scope and out-of-scope lists are explicit and non-overlapping.\n- Out-of-scope list includes real training, multiplayer, and backend services.\n- Document is short (<= 1 page) and easy to reference.\n""",
    "Implement simplified tokenization system": """### Subtasks\n- Define token rules (words, punctuation, whitespace handling).\n- Implement a deterministic tokenizer module.\n- Expose token list in simulation state for UI use.\n- Add a simple token view for inspection.\n\n### Acceptance Criteria\n- Same input text always yields the same tokens.\n- Punctuation and whitespace are handled consistently.\n- Tokens are visible in the UI or debug panel.\n- No external ML libraries are used.\n""",
    "Implement pseudo-embedding representation": """### Subtasks\n- Choose a lightweight representation (e.g., tags + color + small vector).\n- Implement deterministic mapping from token -> representation.\n- Store representations in simulation state.\n- Add a minimal visual (chips or bars) to show differences.\n\n### Acceptance Criteria\n- Each token has a stable representation across runs.\n- Representation is readable and visually distinct.\n- No heavy math or ML libraries are used.\n- Representation data is accessible for other systems (attention, scoring).\n""",
    "Implement attention approximation model": """### Subtasks\n- Define a simple heuristic (recency, keyword match, or role tags).\n- Implement weight calculation for each generated token.\n- Store attention weights in simulation state.\n- Provide a basic debug view or data hook for the UI.\n\n### Acceptance Criteria\n- Attention weights are computed every generation step.\n- Weights are deterministic for the same input and seed.\n- Weights are normalized or clearly scaled for visualization.\n- No tensor math or ML frameworks are used.\n""",
    "Implement multi-layer transformation pipeline": """### Subtasks\n- Define 2-3 transformation stages with clear names.\n- Implement a pipeline that applies stages in order.\n- Capture intermediate outputs for inspection.\n- Add a toggle to enable/disable stages for experimentation.\n\n### Acceptance Criteria\n- Pipeline runs deterministically with ordered stages.\n- Intermediate outputs are inspectable in the UI or logs.\n- Disabling a stage changes results in a visible way.\n- Stages are simple and explainable.\n""",
    "Implement context window mechanics": """### Subtasks\n- Define a max context size parameter.\n- Implement truncation of oldest tokens beyond the window.\n- Track and surface which tokens were dropped.\n- Add a UI indicator for current window bounds.\n\n### Acceptance Criteria\n- Tokens beyond the limit are consistently removed.\n- Dropped tokens are visible or flagged to the player.\n- Window size is configurable for scenarios.\n- Behavior is deterministic and replayable.\n""",
    "Implement sampling logic": """### Subtasks\n- Define temperature and randomness parameters for sampling.\n- Implement a seeded RNG for deterministic sampling.\n- Integrate sampling with per-token probability data.\n- Provide basic controls for sampling parameters.\n\n### Acceptance Criteria\n- Same seed + params produce identical outputs.\n- Higher temperature increases variability in outputs.\n- Sampling choices align with the displayed probabilities.\n- No external ML dependencies are introduced.\n""",
    "Implement deterministic replay support": """### Subtasks\n- Capture all inputs, parameters, and RNG seed for a run.\n- Record any random draws or decisions during generation.\n- Implement a replay mode that uses the recorded data.\n- Add a basic UI control to trigger replay.\n\n### Acceptance Criteria\n- Replay produces identical outputs and visuals.\n- Recorded runs are small enough for local storage.\n- Replay does not re-run simulation logic differently.\n- Determinism is verifiable across sessions.\n""",
    "Implement token-by-token generation": """### Subtasks\n- Implement a stepwise generation loop.\n- Emit token events per step for the UI.\n- Add controls for play, pause, and step.\n- Ensure generation state can be inspected mid-run.\n\n### Acceptance Criteria\n- Tokens appear one at a time during generation.\n- Pause and step controls work reliably.\n- Mid-run state is visible (current token, step count).\n- No blocking or long frame stalls.\n""",
    "Implement per-token probability tracking": """### Subtasks\n- Define a mock probability distribution per step.\n- Store probability data for each generated token.\n- Expose data for visualization and debugging.\n- Ensure sampling uses this data.\n\n### Acceptance Criteria\n- Each generation step has a stored probability distribution.\n- The chosen token is consistent with the distribution.\n- Data is accessible for UI display.\n- Deterministic across seeds.\n""",
    "Implement attention weight tracking": """### Subtasks\n- Store attention weights per generated token.\n- Index weights against source tokens for visualization.\n- Provide a query API for the UI.\n- Include attention data in replay records.\n\n### Acceptance Criteria\n- Attention weights are available for every generated token.\n- UI can render weights without recomputation.\n- Replay preserves attention data.\n- Deterministic across runs.\n""",
    "Implement generation replay mode": """### Subtasks\n- Build UI controls for replay (play/pause/step/speed).\n- Sync replay timeline with visualizations.\n- Allow replay selection from saved runs.\n- Show a clear replay status indicator.\n\n### Acceptance Criteria\n- Replay matches the original run step-by-step.\n- Controls work for step and continuous playback.\n- Visuals stay in sync with replayed data.\n- Replay can be exited without losing state.\n""",
    "Implement objective system": """### Subtasks\n- Define a small objective schema (id, description, evaluation hook).\n- Implement evaluation at end of a run.\n- Add 2-3 sample objectives tied to LLM concepts.\n- Display objective status in the UI.\n\n### Acceptance Criteria\n- Objectives evaluate deterministically.\n- Status (pass/fail) is visible to the player.\n- Objectives are simple and explainable.\n- System is extensible without refactoring.\n""",
    "Implement failure conditions and diagnostics": """### Subtasks\n- Define 3-5 deterministic failure conditions.\n- Implement detection logic for each condition.\n- Create a diagnostics panel with cause + hint.\n- Link diagnostics to the learning concept.\n\n### Acceptance Criteria\n- Failures trigger reliably on defined conditions.\n- Diagnostics explain what happened and why.\n- Tone is non-punitive and instructional.\n- Diagnostics are available in replay.\n""",
    "Implement scoring dimensions": """### Subtasks\n- Define 3-4 scoring axes (e.g., coherence, efficiency, stability).\n- Implement scoring calculation per run.\n- Display score breakdown in the UI.\n- Tie scores to objectives where relevant.\n\n### Acceptance Criteria\n- Scores are deterministic and repeatable.\n- Score breakdown is visible and easy to read.\n- Scores align with the learning goals.\n- No hidden or opaque scoring logic.\n""",
    "Implement unlockable parameters": """### Subtasks\n- Define progression triggers (objective completion or scenario milestones).\n- Lock advanced controls behind progression.\n- Persist unlock state locally.\n- Add a small unlock notification UI.\n\n### Acceptance Criteria\n- Locked controls are clearly disabled and labeled.\n- Unlocks happen at the intended milestones.\n- Unlock state persists across reloads.\n- Unlock messaging is celebratory but brief.\n""",
    "Implement sandbox mode": """### Subtasks\n- Define sandbox entry and exit flow.\n- Disable objectives and scoring in sandbox.\n- Unlock all parameters in sandbox.\n- Add a visible sandbox banner.\n\n### Acceptance Criteria\n- Sandbox mode toggles cleanly on/off.\n- Objectives and scoring are hidden or inactive.\n- All controls are available in sandbox.\n- Sandbox state is obvious to the player.\n""",
    "Implement token probability visualization": """### Subtasks\n- Design a compact probability display (bars or ranked list).\n- Render probabilities for the current step.\n- Highlight the chosen token.\n- Update visualization live during generation.\n\n### Acceptance Criteria\n- Probabilities are legible at a glance.\n- Chosen token is clearly highlighted.\n- Visualization updates per token step.\n- No heavy charting libraries required.\n""",
    "Implement attention heatmap visualization": """### Subtasks\n- Define a heatmap layout (tokens x tokens).\n- Map attention weights to color intensity.\n- Add hover/tooltip for exact values.\n- Update heatmap per generation step.\n\n### Acceptance Criteria\n- Heatmap clearly shows attention focus.\n- Colors are readable and consistent.\n- Hover reveals exact weights.\n- Updates stay in sync with generation.\n""",
    "Implement context window visualization": """### Subtasks\n- Design a timeline or list view of tokens.\n- Mark active window bounds and truncated tokens.\n- Animate or highlight when window slides.\n- Link view to the current generation step.\n\n### Acceptance Criteria\n- Active window is visually distinct.\n- Truncated tokens are clearly shown as dropped.\n- Window updates on overflow events.\n- View is readable on mobile widths.\n""",
    "Implement hallucination indicators": """### Subtasks\n- Define a simple heuristic (low confidence, contradiction, or novelty spike).\n- Flag tokens or spans that meet the heuristic.\n- Provide a brief explanation tooltip.\n- Allow toggling the indicator display.\n\n### Acceptance Criteria\n- Indicators appear only when heuristic triggers.\n- Tooltip explains the likely cause.\n- Indicators do not block reading the output.\n- Behavior is deterministic and replayable.\n""",
    "Implement live parameter sliders": """### Subtasks\n- Identify parameters to expose (temperature, top-k, noise).\n- Implement slider controls with numeric readouts.\n- Apply changes to the next generation step.\n- Persist last-used values locally.\n\n### Acceptance Criteria\n- Slider changes immediately affect subsequent output.\n- Values are displayed and clamped safely.\n- Defaults load on first visit, last values on return.\n- Sliders are accessible via keyboard.\n""",
    "Define playful visual language": """### Subtasks\n- Choose typography, color palette, and UI tone words.\n- Define naming conventions for UI elements (panels, modes).\n- Create a small style guide with examples.\n- Validate against the playful/serious philosophy.\n\n### Acceptance Criteria\n- Style guide includes fonts, colors, and voice examples.\n- UI naming feels playful but credible.\n- Visual direction is distinct from generic dashboards.\n- Guidance is short and actionable for implementation.\n""",
    "Implement explanation panel": """### Subtasks\n- Design a collapsible explanation panel layout.\n- Write short explanations for 5-8 key mechanics.\n- Link explanations to the current UI focus.\n- Add a toggle to show/hide the panel.\n\n### Acceptance Criteria\n- Panel can be shown/hidden quickly.\n- Explanations are concise (<= 3 sentences each).\n- Content updates based on the selected mechanic.\n- Panel does not block core interaction.\n""",
    "Implement tooltips and hover hints": """### Subtasks\n- Identify 8-10 UI terms that need hints.\n- Implement a tooltip component with hover and focus.\n- Write short hint copy for each term.\n- Ensure tooltips are accessible on keyboard navigation.\n\n### Acceptance Criteria\n- Tooltips appear on hover and focus.\n- Copy is concise and educational.\n- Tooltips do not obscure critical controls.\n- Component is reusable across the UI.\n""",
    "Implement failure feedback UI": """### Subtasks\n- Design a failure state panel or banner.\n- Show the cause and a next action.\n- Add a quick retry button.\n- Ensure tone is encouraging.\n\n### Acceptance Criteria\n- Failure UI appears on failure conditions.\n- Cause and next action are clearly visible.\n- Retry works without full reload.\n- Feedback tone is non-punitive.\n""",
    "Implement save and load system": """### Subtasks\n- Define a save file schema for runs and settings.\n- Implement local storage save/load.\n- Build a simple save manager UI.\n- Handle version mismatches gracefully.\n\n### Acceptance Criteria\n- Users can create, list, and load saves.\n- Loaded state matches the saved run.\n- Version changes show a friendly warning.\n- No backend services are required.\n""",
    "Implement scenario state persistence": """### Subtasks\n- Persist current scenario progress locally.\n- Restore scenario state on reload.\n- Provide a reset option for the scenario.\n- Integrate with the save/load system.\n\n### Acceptance Criteria\n- Scenario resumes after refresh.\n- Reset clears scenario progress.\n- No cross-scenario state leakage.\n- Persistence is deterministic.\n""",
    "Implement parameter presets": """### Subtasks\n- Define a preset schema for parameter sets.\n- Build UI to save and apply presets.\n- Add 3-5 default presets with names.\n- Validate preset values against allowed ranges.\n\n### Acceptance Criteria\n- Presets apply all parameters at once.\n- Default presets are available on first run.\n- Invalid preset values are rejected or clamped.\n- Preset UI is simple and discoverable.\n""",
    "Create intro scenario": """### Subtasks\n- Define the learning goal and success condition.\n- Build a short scenario script and setup.\n- Add an objective and a likely failure mode.\n- Provide minimal guidance text.\n\n### Acceptance Criteria\n- Scenario teaches the core loop quickly.\n- Objective is achievable within a few minutes.\n- Failure teaches a clear lesson.\n- Guidance is brief and optional.\n""",
    "Create context window failure scenario": """### Subtasks\n- Design a prompt/setup that exceeds a small context window.\n- Set window size to force truncation.\n- Define an objective that fails due to truncation.\n- Add diagnostics that explain the failure.\n\n### Acceptance Criteria\n- Failure reliably occurs due to context truncation.\n- Diagnostics point to missing context as the cause.\n- Player can resolve by changing inputs or window size.\n- Scenario is short and focused.\n""",
    "Create hallucination tradeoff scenario": """### Subtasks\n- Create a scenario with an accuracy-sensitive objective.\n- Set sampling parameters to encourage hallucinations.\n- Add hints about adjusting randomness.\n- Include indicators for hallucination risk.\n\n### Acceptance Criteria\n- High randomness increases hallucination indicators.\n- Reducing randomness improves accuracy.\n- Objective reflects the accuracy/creativity tradeoff.\n- Scenario is deterministic with a fixed seed.\n""",
    "Create prompt sensitivity scenario": """### Subtasks\n- Build two near-identical prompts with different outcomes.\n- Create a comparison view or toggle.\n- Set an objective to stabilize output.\n- Provide guidance on prompt changes or constraints.\n\n### Acceptance Criteria\n- Small prompt changes cause noticeable output shifts.\n- Players can reduce variance with adjustments.\n- Comparison view makes differences clear.\n- Scenario remains short and replayable.\n""",
    "Define frontend architecture": """### Subtasks\n- Choose the framework and state management approach.\n- Define folder structure and component boundaries.\n- Document data flow between UI and simulation engine.\n- Specify build tooling and dev scripts.\n\n### Acceptance Criteria\n- Architecture doc is concise and diagrammed.\n- UI state vs engine state responsibilities are clear.\n- Choices align with MVP constraints and scope.\n- No backend services are assumed.\n""",
    "Define simulation engine boundaries": """### Subtasks\n- List responsibilities of the engine vs UI.\n- Define a minimal engine API surface.\n- Document inputs/outputs and deterministic requirements.\n- Identify data needed for visualizations.\n\n### Acceptance Criteria\n- Engine API is small and focused.\n- UI does not own simulation logic.\n- Determinism requirements are explicit.\n- Doc is short and implementation-ready.\n""",
    "Implement deterministic simulation clock": """### Subtasks\n- Create a tick-based simulation clock.\n- Drive generation steps off the clock.\n- Add pause, resume, and step controls.\n- Tie timing to a deterministic seed.\n\n### Acceptance Criteria\n- Simulation advances exactly one step per tick.\n- Pause and step controls are reliable.\n- Same seed yields identical timing and output.\n- Clock can be sped up or slowed down.\n""",
    "Implement performance constraints": """### Subtasks\n- Define limits for token counts and steps per second.\n- Enforce limits in the simulation loop.\n- Add warnings when limits are hit.\n- Provide a way to override limits in sandbox mode.\n\n### Acceptance Criteria\n- App remains responsive under heavy inputs.\n- Limits are enforced deterministically.\n- Warnings are visible but non-blocking.\n- Sandbox can relax limits if enabled.\n""",
    "Implement onboarding flow": """### Subtasks\n- Design a 3-4 step onboarding sequence.\n- Implement skip and resume options.\n- Link onboarding to the intro scenario.\n- Track completion locally.\n\n### Acceptance Criteria\n- Onboarding shows on first run only.\n- Users can skip at any time.\n- Completion state is saved locally.\n- Flow transitions to the intro scenario.\n""",
    "Implement settings panel": """### Subtasks\n- Identify settings to expose (speed, audio, hints).\n- Build a settings panel UI.\n- Persist settings locally.\n- Apply changes immediately when possible.\n\n### Acceptance Criteria\n- Settings save and reload correctly.\n- Changes apply without restarting the app.\n- Panel is accessible and easy to find.\n- Defaults are sensible for new users.\n""",
    "Implement error handling": """### Subtasks\n- Define error categories (sim, UI, data).\n- Add error boundaries for UI crashes.\n- Show a friendly error message with recovery action.\n- Log errors to console for debugging.\n\n### Acceptance Criteria\n- App does not fully crash on handled errors.\n- Users see a clear recovery path.\n- Error messages are non-technical and brief.\n- Errors are logged for developers.\n""",
    "Add MVP instrumentation": """### Subtasks\n- Define 5-8 local events to track (run start, failure, objective complete).\n- Implement a local event logger.\n- Add a settings toggle to disable logging.\n- Store logs locally for debugging.\n\n### Acceptance Criteria\n- Events are recorded with timestamps.\n- Logging can be disabled in settings.\n- No external network calls are made.\n- Logs are easy to inspect locally.\n""",
    "Prepare MVP build configuration": """### Subtasks\n- Define build targets and environment variables.\n- Ensure production build works locally.\n- Document build and run steps.\n- Verify build output size is reasonable.\n\n### Acceptance Criteria\n- `npm run build` (or equivalent) succeeds.\n- Build instructions are documented in README.\n- No backend services required for build.\n- Output is stable across runs.\n""",
    "Define explicit non-goals": """### Subtasks\n- List explicit non-goals (training, multiplayer, accounts, real inference).\n- Add brief rationale for each.\n- Align with constraints and scope boundaries.\n- Publish in the main project docs.\n\n### Acceptance Criteria\n- Non-goals list is clear and concise.\n- Includes major scope exclusions from constraints.\n- Document is referenced by planning docs.\n- No ambiguity about what MVP will not do.\n""",
}


def main():
    cmd = ["gh", "issue", "list", "--limit", "200", "--json", "number,title"]
    result = subprocess.run(cmd, check=False, capture_output=True, text=True)
    if result.returncode != 0:
        print(result.stderr.strip(), file=sys.stderr)
        return result.returncode

    issues = {item["title"]: item["number"] for item in json.loads(result.stdout)}
    missing = [title for title in BREAKDOWNS if title not in issues]
    if missing:
        print("Missing issues for titles:", ", ".join(missing), file=sys.stderr)
        return 1

    for title, body in BREAKDOWNS.items():
        number = issues[title]
        edit_cmd = ["gh", "issue", "edit", str(number), "--body", body]
        edit_result = subprocess.run(edit_cmd, check=False, capture_output=True, text=True)
        if edit_result.returncode != 0:
            print(edit_result.stderr.strip(), file=sys.stderr)
            return edit_result.returncode
        print(f"Updated #{number}: {title}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
